{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ccced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
    "\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss  # Resident Set Size (RSS) in bytes\n",
    "\n",
    "# Get inital memory\n",
    "initial_memory = get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dcbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    output = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'])\n",
    "    memory_used = re.findall(r'\\d+', output.decode('utf-8'))\n",
    "    return int(memory_used[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eed3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7575ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_memory = get_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d922637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f83f517ff44898ba1c5f7263ea71bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.78G [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(813694976 bytes read, 962353367 more expected)', IncompleteRead(813694976 bytes read, 962353367 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/urllib3/response.py:705\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    708\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    709\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/urllib3/response.py:830\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    821\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce_content_length\n\u001b[1;32m    822\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[39m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    829\u001b[0m             \u001b[39m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m             \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp_bytes_read, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining)\n\u001b[1;32m    832\u001b[0m \u001b[39mif\u001b[39;00m data:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(813694976 bytes read, 962353367 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 935\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/urllib3/response.py:874\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 874\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    876\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/urllib3/response.py:830\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    821\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce_content_length\n\u001b[1;32m    822\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[39m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    829\u001b[0m             \u001b[39m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m             \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp_bytes_read, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining)\n\u001b[1;32m    832\u001b[0m \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/urllib3/response.py:722\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mexcept\u001b[39;00m (HTTPException, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     \u001b[39mraise\u001b[39;00m ProtocolError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnection broken: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(813694976 bytes read, 962353367 more expected)', IncompleteRead(813694976 bytes read, 962353367 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xm \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39;49m\u001b[39mtransmitter\u001b[39;49m\u001b[39m'\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mtext300M\u001b[39m\u001b[39m'\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m diffusion \u001b[39m=\u001b[39m diffusion_from_config(load_config(\u001b[39m'\u001b[39m\u001b[39mdiffusion\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/diffusion/shap_e_test/shap_e/models/download.py:147\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name, device, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfigs\u001b[39;00m \u001b[39mimport\u001b[39;00m model_from_config\n\u001b[1;32m    146\u001b[0m model \u001b[39m=\u001b[39m model_from_config(load_config(model_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 147\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(load_checkpoint(model_name, device\u001b[39m=\u001b[39;49mdevice, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    148\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/diffusion/shap_e_test/shap_e/models/download.py:133\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint_name, device, progress, cache_dir, chunk_size)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m MODEL_PATHS:\n\u001b[1;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown checkpoint name \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_name\u001b[39m}\u001b[39;00m\u001b[39m. Known names are: \u001b[39m\u001b[39m{\u001b[39;00mMODEL_PATHS\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     )\n\u001b[0;32m--> 133\u001b[0m path \u001b[39m=\u001b[39m fetch_file_cached(\n\u001b[1;32m    134\u001b[0m     MODEL_PATHS[checkpoint_name], progress\u001b[39m=\u001b[39;49mprogress, cache_dir\u001b[39m=\u001b[39;49mcache_dir, chunk_size\u001b[39m=\u001b[39;49mchunk_size\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(path, map_location\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/diffusion/shap_e_test/shap_e/models/download.py:74\u001b[0m, in \u001b[0;36mfetch_file_cached\u001b[0;34m(url, progress, cache_dir, chunk_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m tmp_path \u001b[39m=\u001b[39m local_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.tmp\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(tmp_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39miter_content(chunk_size):\n\u001b[1;32m     75\u001b[0m         \u001b[39mif\u001b[39;00m progress:\n\u001b[1;32m     76\u001b[0m             pbar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/anaconda3/envs/shap_e_env_py39/lib/python3.9/site-packages/requests/models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 818\u001b[0m     \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    819\u001b[0m \u001b[39mexcept\u001b[39;00m DecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(813694976 bytes read, 962353367 more expected)', IncompleteRead(813694976 bytes read, 962353367 more expected))"
     ]
    }
   ],
   "source": [
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('text300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_gpu_memory=gpu_memory\n",
    "gpu_memory = get_gpu_memory_usage()\n",
    "model_gpu_memory = gpu_memory-old_gpu_memory\n",
    "print(f\"GPU Memory Usage for Loading Model: {model_gpu_memory} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start timing deffusion process\")\n",
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d329d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "guidance_scale = 15.0\n",
    "prompt = \"a shark\"\n",
    "\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(texts=[prompt] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6283df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end timing deffusion process\")\n",
    "end_time=time.time()\n",
    "duration=end_time-start_time\n",
    "print(f\"runtime for diffusion process: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_memory = get_gpu_memory_usage()\n",
    "diffusion_gpu_memory = gpu_memory - model_gpu_memory\n",
    "print(f\"GPU Memory Usage for Diffusion: {diffusion_gpu_memory} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d33a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start timing rendering process\")\n",
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633da2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'nerf' # you can change this to 'stf'\n",
    "size = 64 # this is the size of the renders; higher values take longer to render.\n",
    "\n",
    "cameras = create_pan_cameras(size, device)\n",
    "for i, latent in enumerate(latents):\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    display(gif_widget(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5020bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end timing rendering process\")\n",
    "end_time=time.time()\n",
    "duration=end_time-start_time\n",
    "print(f\"runtime for rendering process: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_gpu_memory=gpu_memory\n",
    "gpu_memory = get_gpu_memory_usage()\n",
    "rendering_gpu_memory=gpu_memory-old_gpu_memory\n",
    "print(f\"GPU Memory Usage for Rendering: {rendering_gpu_memory} MiB\")\n",
    "print(f\"Total GPU Memory Usage: {gpu_memory} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of saving the latents as meshes.\n",
    "from shap_e.util.notebooks import decode_latent_mesh\n",
    "\n",
    "for i, latent in enumerate(latents):\n",
    "    t = decode_latent_mesh(xm, latent).tri_mesh()\n",
    "    with open(f'example_mesh_{i}.ply', 'wb') as f:\n",
    "        t.write_ply(f)\n",
    "    with open(f'example_mesh_{i}.obj', 'w') as f:\n",
    "        t.write_obj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a96d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate memory usage after running your code\n",
    "# final_memory = get_memory_usage()\n",
    "\n",
    "# # Calculate the difference\n",
    "# memory_used = final_memory - initial_memory\n",
    "\n",
    "# print(f\"Memory used: {memory_used} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
